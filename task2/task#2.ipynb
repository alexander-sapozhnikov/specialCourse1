{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "random.seed(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    image_paths_list = glob(os.path.join(path, 'train', '*.jpg'))\n",
    "    image_paths_sample = random.sample(image_paths_list, 1000)\n",
    "\n",
    "    for image_path in image_paths_sample:\n",
    "        image_name = os.path.basename(image_path)\n",
    "        image_name_parts = image_name.split('.')\n",
    "        label = image_name_parts[0] if len(image_name_parts) == 3 else None\n",
    "\n",
    "        if label:\n",
    "            y.append(int(label == 'cat'))\n",
    "            \n",
    "        x = image.img_to_array(image.load_img(image_path, target_size=(224, 224)))\n",
    "        x = preprocess_input(x)\n",
    "            \n",
    "        X.append(x)\n",
    "            \n",
    "    return np.array(X), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"/home/alexandr/Downloads\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f2777528fd2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1725\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1727\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1728\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    905\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train =[]\n",
    "test = []\n",
    "trainAns =[]\n",
    "testAns = []\n",
    "for i in range(25):\n",
    "    data = read_dataset(url)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[0], data[1], test_size=0.30)\n",
    "    \n",
    "    preds = model.predict(X_train)\n",
    "    train.append(preds)\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    train.append(preds)\n",
    "    \n",
    "    trainAns.append(y_train)\n",
    "    testAns.append(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.78345072e-08, 2.37372211e-09, 2.98356784e-09, 1.01005448e-07,\n",
       "       1.55912758e-07, 4.56325253e-08, 1.81812680e-08, 1.19760466e-06,\n",
       "       2.06698047e-07, 9.21262711e-08, 8.18391133e-09, 2.76296355e-08,\n",
       "       4.66779113e-07, 1.02110285e-08, 1.86333811e-08, 7.31613099e-08,\n",
       "       5.32881010e-08, 1.54892689e-07, 2.95254097e-08, 1.35709870e-08,\n",
       "       2.65258748e-09, 4.67846036e-07, 6.38064268e-09, 1.45557544e-06,\n",
       "       1.17880866e-07, 7.68078905e-07, 1.91893051e-07, 4.89147389e-08,\n",
       "       6.97013789e-08, 4.46405135e-09, 1.82725515e-07, 1.30176776e-08,\n",
       "       6.05143384e-07, 8.10848064e-08, 8.99276120e-09, 7.21401960e-08,\n",
       "       3.41551569e-08, 6.33767243e-08, 2.82236726e-07, 3.48687053e-07,\n",
       "       2.23260486e-08, 3.35822392e-06, 2.31598818e-07, 2.73340959e-07,\n",
       "       2.08592041e-08, 1.43397813e-06, 1.27704425e-07, 7.49272147e-08,\n",
       "       2.65819182e-07, 1.54517128e-08, 2.82410806e-08, 1.08377812e-07,\n",
       "       1.45148562e-07, 4.61515839e-08, 8.69473865e-07, 4.08064658e-08,\n",
       "       1.64344325e-08, 9.13189808e-08, 2.88613222e-08, 1.41472952e-07,\n",
       "       3.06793112e-07, 1.33087369e-07, 1.72317698e-07, 7.26140172e-07,\n",
       "       1.94452625e-07, 6.02749850e-08, 5.91724756e-07, 2.56980940e-07,\n",
       "       2.51722724e-07, 6.22961807e-08, 1.08867070e-07, 1.41350586e-06,\n",
       "       3.76236677e-08, 5.53399047e-07, 1.19699041e-06, 1.00700845e-07,\n",
       "       1.39950225e-06, 6.20811306e-06, 6.17241449e-06, 9.53998551e-08,\n",
       "       2.37021709e-08, 1.42923973e-09, 3.59640325e-07, 2.35864633e-07,\n",
       "       1.56337862e-07, 9.36454910e-07, 2.93293965e-07, 1.30938522e-08,\n",
       "       1.41487803e-07, 7.10662817e-08, 5.46184964e-09, 6.57375381e-08,\n",
       "       1.62006771e-07, 3.22918567e-07, 1.14089431e-07, 2.22511432e-08,\n",
       "       5.13223704e-08, 3.14351098e-07, 1.59418589e-09, 1.21977209e-07,\n",
       "       3.46520217e-08, 8.79927302e-06, 3.54139559e-07, 4.24295017e-08,\n",
       "       2.05520846e-05, 7.86860141e-08, 7.16782267e-07, 1.32065637e-07,\n",
       "       3.03419023e-09, 1.36002685e-08, 2.32162279e-07, 3.18059591e-07,\n",
       "       7.63000187e-08, 1.21424915e-08, 1.06786445e-07, 1.03412905e-08,\n",
       "       3.04357606e-08, 4.75831596e-09, 5.68481129e-09, 5.66463587e-09,\n",
       "       1.93377332e-08, 2.15968843e-08, 2.43840876e-08, 4.92724830e-08,\n",
       "       1.00463438e-07, 3.60259413e-08, 2.75443028e-07, 1.41333118e-07,\n",
       "       1.66137628e-08, 2.45137741e-08, 1.25955975e-08, 2.79437700e-07,\n",
       "       4.45090969e-08, 5.47196976e-07, 1.07435810e-07, 2.66352345e-07,\n",
       "       2.34414983e-06, 2.09650235e-08, 4.35281642e-08, 4.04459000e-09,\n",
       "       1.22381936e-08, 1.14961480e-08, 2.30474537e-08, 1.82164985e-08,\n",
       "       1.77560944e-09, 2.48526941e-08, 5.12922060e-09, 8.90733487e-10,\n",
       "       2.75380080e-10, 7.02888903e-09, 2.94015393e-07, 2.52438531e-06,\n",
       "       2.63068660e-08, 2.66324545e-08, 6.36473558e-08, 1.05093432e-06,\n",
       "       1.99227642e-07, 1.39670433e-07, 4.59197827e-06, 2.11767852e-02,\n",
       "       8.29884550e-04, 2.46346804e-06, 1.73109470e-07, 7.20161665e-03,\n",
       "       4.99493808e-06, 1.18220423e-05, 4.53514849e-06, 6.16922262e-05,\n",
       "       4.70256899e-04, 6.83981925e-05, 8.46997052e-02, 3.81553982e-05,\n",
       "       4.35071095e-04, 9.41231629e-05, 8.69233475e-07, 3.41336895e-03,\n",
       "       1.93779910e-04, 4.25923266e-04, 7.88731675e-04, 6.77758806e-07,\n",
       "       3.13440760e-05, 1.03875745e-05, 3.30710754e-04, 1.51974027e-05,\n",
       "       4.31899071e-01, 1.27101113e-04, 2.08795173e-05, 6.15590761e-06,\n",
       "       7.38435992e-05, 2.89633434e-04, 1.82469557e-05, 1.15416730e-02,\n",
       "       3.71595175e-04, 1.26211080e-05, 2.17260313e-05, 4.79585594e-07,\n",
       "       9.26583925e-06, 8.00104113e-04, 4.77024805e-05, 1.64838711e-05,\n",
       "       1.30503962e-04, 5.41296686e-07, 7.31764594e-04, 1.37418112e-07,\n",
       "       1.51475456e-06, 1.78312184e-05, 1.48519684e-05, 1.00166887e-01,\n",
       "       8.21423193e-04, 2.77649313e-02, 4.30858599e-05, 2.14719325e-01,\n",
       "       2.06264062e-03, 3.90078896e-03, 1.46855324e-04, 6.24582253e-06,\n",
       "       8.97862265e-05, 3.08473027e-05, 7.53977974e-06, 3.29032104e-04,\n",
       "       6.26927285e-05, 2.95806831e-06, 2.21454638e-06, 5.26834505e-08,\n",
       "       3.73072839e-06, 1.37119787e-03, 1.87415071e-02, 2.64482173e-06,\n",
       "       2.99190970e-06, 2.19397270e-06, 1.90853598e-06, 7.91959883e-06,\n",
       "       6.27979205e-07, 3.89417801e-06, 1.16903975e-05, 6.18169957e-04,\n",
       "       8.17831096e-05, 5.09665369e-07, 1.03930051e-05, 1.01901776e-07,\n",
       "       1.59796309e-06, 9.34029245e-08, 5.36432672e-05, 3.18948692e-03,\n",
       "       2.02779916e-06, 6.84944098e-07, 4.02957723e-02, 1.00448449e-06,\n",
       "       1.88582490e-07, 1.77362921e-07, 3.29780676e-07, 3.64973903e-06,\n",
       "       7.50970742e-07, 2.03543868e-06, 1.64598271e-06, 2.01838382e-04,\n",
       "       6.12738659e-05, 4.87158650e-06, 5.21633083e-08, 4.41323067e-07,\n",
       "       7.29909459e-07, 4.87786203e-08, 1.17056865e-04, 2.60430539e-07,\n",
       "       1.59639654e-07, 8.46672719e-07, 7.85558825e-07, 5.27667231e-04,\n",
       "       2.44033299e-06, 4.04028196e-06, 6.51444395e-07, 9.57449447e-06,\n",
       "       1.44510586e-05, 1.29559761e-04, 3.17974373e-05, 3.82540282e-04,\n",
       "       1.04431054e-02, 1.86026818e-06, 1.60662114e-06, 3.49958640e-07,\n",
       "       3.31628826e-07, 1.61996797e-06, 1.39021722e-06, 6.10280750e-08,\n",
       "       3.41481245e-06, 2.66414713e-06, 1.05751671e-04, 5.50222421e-06,\n",
       "       3.76239927e-06, 1.93380572e-07, 6.96438320e-08, 2.59057269e-03,\n",
       "       2.66512552e-06, 8.44020724e-06, 1.46040882e-04, 2.60350589e-07,\n",
       "       1.24465873e-06, 5.50635264e-07, 3.89231602e-04, 3.12908844e-04,\n",
       "       8.80477060e-08, 4.30463043e-09, 8.42080610e-08, 3.66838329e-08,\n",
       "       6.39710862e-08, 2.26446673e-07, 6.70937723e-08, 6.66749884e-07,\n",
       "       1.95870378e-08, 6.77208618e-08, 5.24275343e-08, 1.17859008e-06,\n",
       "       1.27386571e-07, 4.98709142e-07, 7.22259728e-08, 1.58527911e-07,\n",
       "       3.46877442e-07, 1.62978239e-07, 2.00086944e-07, 3.44094673e-07,\n",
       "       4.31652779e-07, 3.87774222e-08, 7.66121673e-08, 4.07700576e-08,\n",
       "       3.44542421e-08, 2.47969524e-07, 1.21324204e-08, 4.49691235e-08,\n",
       "       8.36632985e-09, 9.40730693e-09, 2.51001552e-06, 3.98582088e-05,\n",
       "       2.15767901e-08, 3.24495346e-08, 1.23942385e-07, 3.77639003e-06,\n",
       "       4.28382873e-06, 2.98402341e-07, 1.56065596e-08, 9.64755827e-07,\n",
       "       1.36204801e-06, 2.93860808e-06, 5.13974192e-06, 4.52118074e-05,\n",
       "       2.59187338e-08, 1.40810118e-06, 3.09098345e-06, 7.91197010e-07,\n",
       "       1.01305329e-06, 6.64485287e-06, 1.18912729e-04, 1.32047338e-04,\n",
       "       2.02807951e-06, 8.38060146e-07, 3.29829927e-04, 1.96755266e-07,\n",
       "       1.14568629e-05, 1.32734854e-07, 4.23826151e-07, 2.32821799e-06,\n",
       "       1.21144836e-07, 1.18309636e-07, 1.75531397e-06, 9.02803549e-07,\n",
       "       1.84833079e-05, 4.95821814e-06, 2.32174784e-06, 2.33334021e-07,\n",
       "       1.19589104e-05, 8.91075558e-07, 2.55791889e-07, 5.90404397e-06,\n",
       "       2.10611871e-03, 3.46942979e-05, 5.41177224e-06, 5.80529118e-08,\n",
       "       2.06287359e-06, 3.73187063e-08, 4.06879099e-06, 4.28359181e-06,\n",
       "       5.60057458e-07, 6.38295342e-06, 1.41187297e-06, 8.55526366e-07,\n",
       "       7.38062795e-07, 2.54271818e-05, 3.29224622e-06, 1.97202188e-09,\n",
       "       1.07089484e-08, 6.69248692e-08, 2.52101913e-08, 2.81258803e-08,\n",
       "       1.57514499e-08, 4.62054661e-09, 4.95738783e-09, 9.38249372e-08,\n",
       "       4.87824821e-08, 4.08619805e-09, 1.65610228e-08, 6.50512488e-09,\n",
       "       1.73652079e-08, 1.25031807e-08, 5.65185587e-08, 2.26693619e-09,\n",
       "       1.08378684e-08, 2.07323954e-08, 1.35227793e-07, 1.52791806e-08,\n",
       "       1.94816696e-08, 1.78749730e-07, 2.23980365e-07, 1.24848070e-08,\n",
       "       1.68569113e-06, 3.70236791e-07, 3.10651878e-08, 3.55854324e-09,\n",
       "       2.20888410e-07, 7.82891107e-07, 3.90877997e-09, 4.80509641e-08,\n",
       "       7.29049248e-08, 1.23933705e-06, 1.56697126e-07, 8.03136313e-09,\n",
       "       1.62787483e-09, 1.14567641e-07, 3.62322766e-07, 1.99382475e-06,\n",
       "       6.44378497e-06, 1.68298996e-07, 3.93452183e-07, 2.04206660e-08,\n",
       "       2.48799324e-07, 4.42294876e-08, 4.02301993e-07, 1.91336611e-07,\n",
       "       2.83174550e-08, 3.53035205e-07, 1.11859517e-08, 8.36081071e-09,\n",
       "       1.06419030e-07, 4.51240361e-08, 2.02899528e-07, 5.31199174e-09,\n",
       "       1.73220613e-07, 1.47405913e-07, 1.68566689e-08, 6.60919227e-07,\n",
       "       3.97311055e-07, 1.64198628e-08, 9.98489846e-10, 7.76829339e-08,\n",
       "       1.35842724e-08, 3.00295788e-08, 2.44224889e-08, 3.03692900e-07,\n",
       "       2.88985177e-07, 4.49476865e-08, 2.82423343e-07, 4.41442182e-07,\n",
       "       7.42351602e-08, 1.32718910e-07, 1.95703748e-07, 3.81971950e-05,\n",
       "       2.80464381e-08, 1.26586394e-06, 3.35741035e-08, 1.04385936e-06,\n",
       "       5.11954674e-08, 6.17737896e-08, 1.78979505e-07, 5.48247669e-08,\n",
       "       8.08501591e-06, 2.16394778e-08, 3.09133252e-09, 3.99350064e-08,\n",
       "       6.87766857e-08, 9.67840119e-09, 1.13568621e-08, 6.08392448e-09,\n",
       "       4.22641079e-07, 1.94690500e-07, 1.49215111e-08, 1.22671793e-07,\n",
       "       1.14302487e-08, 4.56578375e-09, 1.44609906e-08, 2.38970923e-08,\n",
       "       3.85140339e-07, 1.24132825e-07, 7.90641064e-09, 1.94213658e-06,\n",
       "       3.33605819e-08, 1.16851400e-08, 6.09860180e-08, 7.23946147e-09,\n",
       "       5.96143890e-09, 4.45906096e-07, 6.55439791e-08, 6.42223199e-08,\n",
       "       6.79917207e-07, 3.29933734e-08, 2.25710153e-07, 1.14683560e-08,\n",
       "       4.57279299e-08, 1.00924765e-07, 2.34282229e-08, 1.21241584e-07,\n",
       "       3.53608200e-08, 8.49690984e-10, 4.92899899e-09, 8.86555895e-09,\n",
       "       1.20586083e-06, 2.24187033e-07, 4.92931213e-06, 4.96411587e-07,\n",
       "       3.16757820e-08, 3.53503168e-07, 8.12632450e-08, 7.32591559e-07,\n",
       "       7.58839158e-10, 1.69512671e-08, 3.14726037e-07, 2.85688270e-06,\n",
       "       6.72955935e-07, 5.27092290e-08, 1.01099067e-08, 2.63132414e-08,\n",
       "       8.77114630e-08, 5.08454328e-08, 1.72579689e-07, 7.00106924e-08,\n",
       "       3.44391964e-08, 1.99453769e-08, 7.45592601e-08, 3.38310868e-09,\n",
       "       2.68718630e-08, 4.01457669e-07, 6.15526119e-08, 1.87012643e-06,\n",
       "       7.88926791e-09, 1.60805357e-07, 1.81050581e-07, 1.20168161e-05,\n",
       "       6.73618246e-08, 3.42059906e-07, 1.62677921e-07, 2.90093407e-08,\n",
       "       9.81915793e-09, 1.89017051e-08, 9.61587610e-09, 8.61874483e-09,\n",
       "       8.71340262e-08, 2.04031458e-08, 8.02228755e-08, 7.86477514e-08,\n",
       "       3.78295567e-07, 2.06975983e-07, 6.74446810e-08, 6.07540755e-07,\n",
       "       4.37622489e-08, 2.11695621e-07, 1.94031945e-05, 2.50834598e-09,\n",
       "       4.64113548e-09, 9.19349361e-08, 4.66943568e-08, 5.45452622e-07,\n",
       "       2.33825645e-07, 4.85655427e-09, 6.78585650e-08, 6.84637143e-07,\n",
       "       7.18021909e-09, 1.22983534e-08, 1.90727746e-07, 1.60134537e-08,\n",
       "       1.24274957e-09, 2.29150260e-07, 1.73488601e-07, 6.22149221e-09,\n",
       "       1.76193765e-07, 2.96990912e-08, 3.22456231e-08, 4.44264039e-08,\n",
       "       1.79786974e-09, 9.03531827e-08, 6.03974968e-08, 2.99674525e-07,\n",
       "       9.99088954e-08, 1.33016442e-07, 1.27309994e-08, 1.49370379e-07,\n",
       "       2.20322449e-09, 4.86942895e-07, 3.33902619e-07, 2.26903438e-07,\n",
       "       4.39725795e-07, 2.34122197e-07, 6.08657080e-09, 3.86357641e-07,\n",
       "       1.19348181e-06, 6.55482921e-08, 3.97862721e-07, 6.18785023e-08,\n",
       "       9.60550128e-09, 1.67253607e-08, 6.56092380e-09, 1.81409465e-08,\n",
       "       1.48528031e-07, 1.16702063e-06, 6.00098645e-07, 2.81592293e-07,\n",
       "       2.30519817e-08, 1.49488297e-08, 4.03381195e-09, 4.14739034e-08,\n",
       "       1.01955288e-06, 1.19950316e-08, 8.75826558e-08, 5.77114690e-07,\n",
       "       4.22513324e-07, 1.71321869e-06, 2.59585363e-06, 9.58646584e-09,\n",
       "       2.79861445e-09, 1.51813442e-08, 1.16368597e-07, 1.52713149e-08,\n",
       "       6.09223871e-09, 1.52841952e-08, 1.09864731e-08, 1.33472753e-07,\n",
       "       5.34338767e-07, 1.02803007e-07, 3.74717871e-07, 1.56074833e-08,\n",
       "       3.04338457e-08, 1.89301446e-07, 1.37706479e-07, 7.14602564e-08,\n",
       "       2.28055050e-08, 4.04581044e-08, 6.80932359e-08, 8.85258586e-08,\n",
       "       9.27736494e-07, 1.57608877e-08, 3.85272955e-07, 6.23294554e-08,\n",
       "       1.33021061e-08, 2.74073130e-07, 8.64338176e-07, 3.89650729e-08,\n",
       "       2.79325114e-08, 3.52253124e-08, 4.58439047e-08, 4.33800516e-08,\n",
       "       2.69031464e-08, 1.73045667e-08, 3.46421736e-08, 2.12790141e-08,\n",
       "       1.36343260e-07, 2.64450630e-08, 1.08624192e-07, 4.52091768e-07,\n",
       "       7.22454629e-08, 2.17947718e-06, 7.04176131e-08, 2.03110062e-08,\n",
       "       3.37711477e-08, 5.38176117e-08, 4.96925203e-08, 8.08207403e-07,\n",
       "       1.70126043e-07, 2.53093457e-08, 8.35007029e-07, 3.20011351e-09,\n",
       "       9.18204751e-05, 5.14934015e-07, 9.55394270e-08, 2.86640289e-09,\n",
       "       1.01112811e-07, 1.38308209e-07, 1.15583546e-07, 4.11610444e-08,\n",
       "       3.62578376e-08, 9.41213507e-09, 1.20750752e-08, 5.17883236e-10,\n",
       "       6.93924278e-08, 9.60706288e-08, 3.71956418e-07, 2.77063265e-07,\n",
       "       9.22345311e-09, 1.76032950e-06, 4.66228745e-09, 4.55783578e-07,\n",
       "       2.02060608e-08, 6.78837742e-09, 7.75421881e-07, 1.53250483e-06,\n",
       "       7.61746378e-07, 1.44212365e-07, 1.58080127e-07, 4.76426919e-07,\n",
       "       5.00339965e-08, 2.17280274e-08, 4.83288218e-08, 1.02247490e-08,\n",
       "       5.66673589e-06, 2.79575474e-09, 5.78022750e-08, 1.55844599e-08,\n",
       "       1.44525805e-08, 8.11140044e-09, 9.66889431e-08, 3.63692116e-08,\n",
       "       3.07779396e-06, 1.41763195e-07, 3.41065430e-07, 8.98607500e-09,\n",
       "       1.49270054e-08, 4.78912980e-08, 4.93799973e-07, 1.56502917e-08,\n",
       "       2.11749622e-08, 1.04265794e-07, 8.32047107e-08, 1.25451276e-08,\n",
       "       4.37182649e-07, 8.82860363e-09, 1.30018225e-06, 7.76823811e-07,\n",
       "       4.35578116e-08, 1.20109377e-07, 7.66215891e-08, 2.77851644e-08,\n",
       "       9.40675093e-09, 1.83638562e-08, 6.29351575e-07, 6.84825494e-08,\n",
       "       1.50867294e-07, 1.87670626e-08, 4.60562646e-08, 6.95354885e-09,\n",
       "       8.17924928e-08, 4.48944775e-08, 4.41079839e-09, 1.36921940e-07,\n",
       "       6.81271128e-09, 4.75012065e-08, 1.30080409e-07, 3.96098532e-09,\n",
       "       1.46702178e-07, 1.82266469e-08, 3.17167093e-08, 4.01053768e-08,\n",
       "       2.55977261e-06, 1.64632240e-07, 6.17570564e-08, 1.11844436e-07,\n",
       "       1.35636125e-08, 5.72417456e-08, 1.43229162e-08, 3.60358541e-07,\n",
       "       2.78421567e-07, 1.48735552e-07, 4.86237525e-07, 4.21273469e-08,\n",
       "       1.44957369e-07, 5.56381252e-08, 9.66969651e-08, 1.69437353e-08,\n",
       "       7.34092387e-09, 2.60980642e-08, 1.56917721e-07, 3.01550330e-07,\n",
       "       2.47915045e-06, 2.84608408e-08, 7.52690497e-08, 1.27044757e-06,\n",
       "       6.34911856e-08, 1.40797347e-08, 9.79012000e-08, 4.89945258e-08,\n",
       "       1.80034206e-07, 2.86811606e-08, 1.58731837e-07, 1.10575577e-07,\n",
       "       9.24508203e-09, 2.76000858e-08, 1.74631243e-08, 1.31464361e-07,\n",
       "       3.77711558e-07, 6.45561329e-08, 2.27014695e-07, 7.03782703e-08,\n",
       "       3.81039200e-08, 2.46116343e-07, 1.44590784e-09, 8.26892347e-07,\n",
       "       2.32919710e-08, 2.90585319e-07, 1.64630141e-08, 4.65159289e-09,\n",
       "       2.16921592e-07, 1.59408264e-05, 3.29663834e-08, 4.55051925e-08,\n",
       "       6.84574104e-08, 7.37796757e-09, 3.74731091e-09, 5.62832099e-07,\n",
       "       1.38695686e-08, 9.34334423e-07, 7.15861415e-09, 1.38587069e-07,\n",
       "       6.42556142e-09, 1.46988555e-09, 3.13143772e-07, 3.46054563e-08,\n",
       "       1.10308402e-08, 8.87260896e-08, 1.29889823e-08, 1.40379782e-07,\n",
       "       4.81919464e-08, 1.67291034e-07, 2.13487343e-08, 7.08500778e-08,\n",
       "       7.52370468e-08, 1.77128470e-07, 6.26475085e-08, 9.36685680e-08,\n",
       "       1.53237551e-07, 2.96958618e-08, 4.14682745e-07, 1.46627343e-07,\n",
       "       5.97046608e-08, 2.57239122e-07, 1.70492143e-07, 7.59587166e-08,\n",
       "       2.57617955e-07, 1.05250052e-07, 1.56955946e-06, 2.51621202e-07,\n",
       "       5.93606853e-09, 1.02668480e-07, 2.43852412e-07, 2.35803668e-07,\n",
       "       5.26539168e-08, 1.06599565e-08, 1.08123238e-07, 3.75277551e-07,\n",
       "       6.88189402e-06, 1.21572256e-07, 4.83889622e-08, 2.43730689e-08,\n",
       "       2.17484384e-07, 7.51633479e-07, 6.34285442e-08, 1.02929356e-07,\n",
       "       9.30665536e-08, 8.38422594e-08, 1.06726077e-07, 9.86110109e-08,\n",
       "       6.18603906e-08, 3.98364861e-08, 4.96351902e-07, 9.82447812e-09,\n",
       "       1.40807487e-07, 1.88666320e-07, 2.44318215e-07, 7.14404624e-09,\n",
       "       2.88040474e-06, 2.20601783e-06, 4.53010784e-08, 3.53226000e-08,\n",
       "       8.61074398e-07, 1.57526543e-08, 3.68254272e-09, 1.21510354e-07,\n",
       "       1.97191824e-07, 1.57549369e-08, 5.25574194e-07, 5.50271686e-08,\n",
       "       2.23541420e-07, 9.91795304e-08, 6.00709527e-09, 5.21804955e-09,\n",
       "       2.86283267e-07, 3.58078864e-08, 1.39107982e-07, 1.35701077e-08,\n",
       "       6.11913492e-07, 4.06433287e-09, 1.46165844e-08, 4.51681048e-09,\n",
       "       1.01357273e-07, 2.08379720e-06, 2.32591987e-07, 1.95658387e-07,\n",
       "       1.44094203e-08, 1.95340597e-08, 5.90980960e-07, 1.27961158e-07,\n",
       "       6.73226225e-07, 3.62433177e-08, 1.87037035e-08, 2.19757354e-07,\n",
       "       2.32412454e-08, 2.39249488e-07, 1.03714353e-06, 1.19181545e-08,\n",
       "       6.66662743e-07, 1.04446521e-07, 6.12896067e-10, 2.43466928e-07,\n",
       "       1.13460104e-07, 5.95633082e-07, 7.89239536e-08, 5.19337618e-07,\n",
       "       5.74738905e-08, 1.88586711e-08, 9.19939624e-10, 2.74908429e-09,\n",
       "       3.53700425e-09, 5.12857490e-09, 9.94002036e-09, 3.55222768e-10,\n",
       "       1.12812726e-09, 1.40874647e-07, 3.77300289e-08, 1.24110633e-08,\n",
       "       4.15641033e-08, 5.16249099e-10, 9.92174165e-09, 1.23856649e-08,\n",
       "       1.71295707e-08, 1.66193956e-07, 1.15468843e-07, 3.20894138e-08,\n",
       "       3.59657264e-08, 3.09455537e-08, 1.00414965e-07, 1.70278625e-07,\n",
       "       4.79916089e-08, 2.33879049e-08, 4.90387464e-08, 5.47250750e-07,\n",
       "       1.14915181e-07, 2.93833331e-08, 9.60033120e-08, 1.28758188e-07,\n",
       "       5.41429301e-09, 4.19491037e-08, 2.79417054e-06, 1.23460126e-07,\n",
       "       1.49508338e-07, 6.95538915e-08, 3.76375027e-07, 6.24122549e-08,\n",
       "       7.04619119e-09, 5.11214076e-08, 3.40193402e-08, 4.00491196e-08,\n",
       "       2.16254925e-08, 1.37065115e-09, 4.10295158e-08, 1.36148026e-08,\n",
       "       4.74814605e-08, 1.33617100e-08, 5.39199732e-07, 3.10052400e-07,\n",
       "       1.56657243e-05, 5.03880599e-08, 1.23736413e-07, 4.68021426e-07,\n",
       "       1.48798392e-07, 1.01373951e-06, 4.27914119e-06, 2.37395466e-06,\n",
       "       1.23520181e-07, 1.67378094e-08, 1.18650988e-07, 7.84619658e-08,\n",
       "       1.39518863e-06, 1.25925945e-08, 8.79579858e-08, 7.64637025e-06,\n",
       "       4.08305425e-07, 9.84115616e-08, 1.55203637e-07, 3.12646932e-07,\n",
       "       3.40263462e-08, 3.15820921e-06, 1.98812359e-07, 1.23078712e-06,\n",
       "       6.03997137e-07, 5.04812725e-08, 2.67719406e-05, 1.05768649e-07],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:52:03] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=16, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "clf = XGBClassifier()\n",
    "\n",
    "clf.fit(preds,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n",
       "       1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-b4618f0b8c86>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-b4618f0b8c86>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    kaggle competitions download -c dogs-vs-cats\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "kaggle competitions download -c dogs-vs-cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
